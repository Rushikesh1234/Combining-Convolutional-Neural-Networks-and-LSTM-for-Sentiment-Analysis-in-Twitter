{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1EDX2K2aHPb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import string\n",
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "import re\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8csHgZGVYFs",
        "outputId": "47365c37-285e-4a70-eab8-4f8fe64913ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tHSbnS6GaUXq",
        "outputId": "90d2333a-0633-471e-ae75-562a596a548d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Sentiment                                      SentimentText\n",
              "0  Negative           is so sad for my APL friend.............\n",
              "1  Negative                   I missed the New Moon trailer...\n",
              "2  Positive                            omg its already 7:30 :O\n",
              "3  Negative  .. Omgaga. Im sooo  im gunna CRy. I've been at...\n",
              "4  Negative       i think mi bf is cheating on me!!!       T_T"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e59ccd94-5497-46e7-ac7a-3b76ecfdd2a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Negative</td>\n",
              "      <td>is so sad for my APL friend.............</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Negative</td>\n",
              "      <td>I missed the New Moon trailer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Positive</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Negative</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Negative</td>\n",
              "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e59ccd94-5497-46e7-ac7a-3b76ecfdd2a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e59ccd94-5497-46e7-ac7a-3b76ecfdd2a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e59ccd94-5497-46e7-ac7a-3b76ecfdd2a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "data= pd.read_csv('/content/drive/MyDrive/Dataset/dataset.csv', encoding=\"latin1\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Sentiment'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRo5yItOTukR",
        "outputId": "58da30dc-31f5-4a4d-c123-68745a24bf00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Negative', 'Positive', nan], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUqlXYxDdHVt"
      },
      "outputs": [],
      "source": [
        "data['Sentiment'].replace({\"1.0\":1,\"0.0\":0},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Sentiment\"].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojy9pNbyR7EP",
        "outputId": "7a45f11a-070f-4ad6-a12f-5e1a1a716edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Negative', 'Positive', nan], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKZgUQfhXQDB"
      },
      "outputs": [],
      "source": [
        "Edited_Tweets = data['SentimentText'].copy()\n",
        "data['Tweets_without_stopwords'] = Edited_Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a9CaYQ4XhUb"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess tweets \n",
        "def preprocess_tweet_data(data,name):\n",
        "    # Proprocessing the data\n",
        "    data[name]=data[name].str.lower()\n",
        "    # Code to remove the Hashtags from the text\n",
        "    data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n",
        "    # Code to remove the links from the text\n",
        "    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
        "    # Code to remove the Special characters from the text \n",
        "    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
        "    # Code to substitute the multiple spaces with single spaces\n",
        "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
        "    # Code to remove all the single characters in the text\n",
        "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
        "    # Remove the twitter handlers\n",
        "    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\n",
        "\n",
        "# Function to tokenize and remove the stopwords    \n",
        "def rem_stopwords_tokenize(data,name):\n",
        "      \n",
        "    def getting(sen):\n",
        "        example_sent = sen\n",
        "        \n",
        "        filtered_sentence = [] \n",
        "\n",
        "        stop_words = set(stopwords.words('english')) \n",
        "\n",
        "        word_tokens = word_tokenize(example_sent) \n",
        "        \n",
        "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "        \n",
        "        return filtered_sentence\n",
        "    # Using \"getting(sen)\" function to append edited sentence to data\n",
        "    x=[]\n",
        "    for i in data[name].values:\n",
        "        x.append(getting(i))\n",
        "    data[name]=x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjg5YM0WXobC"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def Lemmatization(data,name):\n",
        "    def getting2(sen):\n",
        "        \n",
        "        example = sen\n",
        "        output_sentence =[]\n",
        "        word_tokens2 = word_tokenize(example)\n",
        "        lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n",
        "        \n",
        "        # Remove characters which have length less than 2  \n",
        "        without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n",
        "        # Remove numbers\n",
        "        cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]\n",
        "        \n",
        "        return cleaned_data_title\n",
        "    x=[]\n",
        "    for i in data[name].values:\n",
        "        x.append(getting2(i))\n",
        "    data[name]=x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfdIGB_HXsS0"
      },
      "outputs": [],
      "source": [
        "def make_sentences(data,name):\n",
        "    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n",
        "    # Removing double spaces if created\n",
        "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwHG7WHyYcel",
        "outputId": "d98b8496-ba37-4e7c-8a02-af63d026250f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl6tMY0dbx5Y",
        "outputId": "e6073c3a-8485-402f-977f-7745ea03b088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDCD8sy2Yjfh",
        "outputId": "5ea5a291-95ae-4339-d092-b320247f3867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y467ATfXvg8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "f125676f-0870-4130-efe1-5ea757bdd85d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-d3d5f7f6f2a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using the preprocessing function to preprocess the tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocess_tweet_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Tweets_without_stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Using tokenizer and removing the stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrem_stopwords_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Tweets_without_stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Converting all the texts back to sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-b7e28487a97a>\u001b[0m in \u001b[0;36mpreprocess_tweet_data\u001b[0;34m(data, name)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Code to remove the Hashtags from the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\B#\\S+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Code to remove the links from the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"http\\S+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                     \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 )\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-b7e28487a97a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Code to remove the Hashtags from the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\B#\\S+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Code to remove the links from the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"http\\S+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ],
      "source": [
        "# Using the preprocessing function to preprocess the tweets\n",
        "preprocess_tweet_data(data,'Tweets_without_stopwords')\n",
        "# Using tokenizer and removing the stopwords\n",
        "rem_stopwords_tokenize(data,'Tweets_without_stopwords')\n",
        "# Converting all the texts back to sentences\n",
        "make_sentences(data,'Tweets_without_stopwords')\n",
        "\n",
        "#Edits After Lemmatization\n",
        "final_Edit = data['Tweets_without_stopwords'].copy()\n",
        "data[\"After_lemmatization\"] = final_Edit\n",
        "\n",
        "# Using the Lemmatization function to lemmatize the tweets\n",
        "Lemmatization(data,'After_lemmatization')\n",
        "# Converting all the texts back to sentences\n",
        "make_sentences(data,'After_lemmatization')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFnDXB4Sc0-v",
        "outputId": "1850b1dd-f8d2-4663-8ad0-e4ea222857f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF4jPiTrejGT"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTnvl-7KfEIC",
        "outputId": "8acd96d0-e4ed-4c5b-df09-73ae66167b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('sentiwordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "to7dyFipfHuZ",
        "outputId": "4b1551ec-f92d-4677-958b-70b4fed3f826"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'After_lemmatization'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-c692a7b81bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpostagging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'After_lemmatization'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpostagging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'After_lemmatization'"
          ]
        }
      ],
      "source": [
        "pos=neg=obj=count=0\n",
        "\n",
        "postagging = []\n",
        "\n",
        "for tweet in data['After_lemmatization']:\n",
        "    li = word_tokenize(tweet)\n",
        "    postagging.append(nltk.pos_tag(li))\n",
        "\n",
        "data['pos_tags'] = postagging\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "         return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "\n",
        "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
        "def get_sentiment(word,tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    \n",
        "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "        return []\n",
        "\n",
        "    #Lemmatization\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "    if not lemma:\n",
        "        return []\n",
        "\n",
        "    synsets = wn.synsets(word, pos=wn_tag)\n",
        "    if not synsets:\n",
        "        return []\n",
        "\n",
        "    # Take the first sense, the most common\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
        "\n",
        "    pos=neg=obj=count=0\n",
        "  \n",
        "senti_score = []\n",
        "\n",
        "for pos_val in data['pos_tags']:\n",
        "    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
        "    for score in senti_val:\n",
        "        try:\n",
        "            pos = pos + score[1]  #positive score is stored at 2nd position\n",
        "            neg = neg + score[2]  #negative score is stored at 3rd position\n",
        "        except:\n",
        "            continue\n",
        "    senti_score.append(pos - neg)\n",
        "    pos=neg=0    \n",
        "data['senti_score'] = senti_score\n",
        "print(data['senti_score'])\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "Hq7OIurJfUQ5",
        "outputId": "f36a9b15-e2a4-4fd1-d666-d6e83322f05c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-24821662-c61c-4925-bc0e-f47207932d11\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentSource</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Tweets_without_stopwords</th>\n",
              "      <th>After_lemmatization</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>senti_score</th>\n",
              "      <th>Overall Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "      <td>sad apl friend</td>\n",
              "      <td>sad apl friend</td>\n",
              "      <td>[(sad, JJ), (apl, NN), (friend, NN)]</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "      <td>missed new moon trailer</td>\n",
              "      <td>missed new moon trailer</td>\n",
              "      <td>[(missed, VBN), (new, JJ), (moon, NN), (traile...</td>\n",
              "      <td>0.375</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "      <td>omg already 7 30</td>\n",
              "      <td>omg already</td>\n",
              "      <td>[(omg, NNS), (already, RB)]</td>\n",
              "      <td>0.125</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "      <td>omgaga im sooo im gunna cryve dentist since 11...</td>\n",
              "      <td>omgaga sooo gunna cryve dentist since 11was su...</td>\n",
              "      <td>[(omgaga, JJ), (sooo, NN), (gunna, NN), (cryve...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "      <td>think mi bf cheating t_t</td>\n",
              "      <td>think cheating t_t</td>\n",
              "      <td>[(think, NN), (cheating, VBG), (t_t, NN)]</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24821662-c61c-4925-bc0e-f47207932d11')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24821662-c61c-4925-bc0e-f47207932d11 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24821662-c61c-4925-bc0e-f47207932d11');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   ItemID  Sentiment SentimentSource  \\\n",
              "0       1          0    Sentiment140   \n",
              "1       2          0    Sentiment140   \n",
              "2       3          1    Sentiment140   \n",
              "3       4          0    Sentiment140   \n",
              "4       5          0    Sentiment140   \n",
              "\n",
              "                                       SentimentText  \\\n",
              "0                       is so sad for my APL frie...   \n",
              "1                     I missed the New Moon trail...   \n",
              "2                            omg its already 7:30 :O   \n",
              "3            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
              "4           i think mi bf is cheating on me!!!   ...   \n",
              "\n",
              "                            Tweets_without_stopwords  \\\n",
              "0                                    sad apl friend    \n",
              "1                           missed new moon trailer    \n",
              "2                                  omg already 7 30    \n",
              "3  omgaga im sooo im gunna cryve dentist since 11...   \n",
              "4                          think mi bf cheating t_t    \n",
              "\n",
              "                                 After_lemmatization  \\\n",
              "0                                    sad apl friend    \n",
              "1                           missed new moon trailer    \n",
              "2                                       omg already    \n",
              "3  omgaga sooo gunna cryve dentist since 11was su...   \n",
              "4                                think cheating t_t    \n",
              "\n",
              "                                            pos_tags  senti_score  \\\n",
              "0               [(sad, JJ), (apl, NN), (friend, NN)]       -0.500   \n",
              "1  [(missed, VBN), (new, JJ), (moon, NN), (traile...        0.375   \n",
              "2                        [(omg, NNS), (already, RB)]        0.125   \n",
              "3  [(omgaga, JJ), (sooo, NN), (gunna, NN), (cryve...        0.000   \n",
              "4          [(think, NN), (cheating, VBG), (t_t, NN)]        0.000   \n",
              "\n",
              "  Overall Sentiment  \n",
              "0          Negative  \n",
              "1          Positive  \n",
              "2          Positive  \n",
              "3           Neutral  \n",
              "4           Neutral  "
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "overall=[]\n",
        "for i in range(len(data)):\n",
        "    if data['senti_score'][i]>= 0.05:\n",
        "        overall.append('Positive')\n",
        "    elif data['senti_score'][i]<= -0.05:\n",
        "        overall.append('Negative')\n",
        "    else:\n",
        "        overall.append('Neutral')\n",
        "data['Overall Sentiment']=overall\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDrwo-2yf5KF"
      },
      "outputs": [],
      "source": [
        "data.to_csv(\"/drive/MyDrive/sentiment-cnn/cleaned_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjg1aYG4gocb"
      },
      "outputs": [],
      "source": [
        "# Read file from gdrive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data=pd.read_csv('/drive/MyDrive/sentiment-cnn/cleaned_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(subset=[\"After_lemmatization\"], inplace=True)"
      ],
      "metadata": {
        "id": "CWd-yBsKU9ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDHf5Dqng7og"
      },
      "outputs": [],
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8uO0P77hGyG"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(data['After_lemmatization'].values)\n",
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dR6qVynhMiv"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = tokenizer.texts_to_sequences(data['After_lemmatization'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4iqX9LAhO0u"
      },
      "outputs": [],
      "source": [
        "#Y = pd.get_dummies(data['sentiment']).values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "mlb = LabelEncoder()\n",
        "sentiment = data['Sentiment'].to_numpy()\n",
        "mlb.fit(sentiment)\n",
        "Y = mlb.transform(sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "4I1nirtW4oI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZKYLPlZhbeH",
        "outputId": "9953c03c-9c8b-44d9-c212-c58138dc1e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179768 words total, with a vocabulary size of 1\n"
          ]
        }
      ],
      "source": [
        "## build training vocabulary and get maximum training sentence length and total number of words training data\n",
        "all_training_words = ''.join([ word for tokens in train[\"After_lemmatization\"] for word in tokens])\n",
        "training_sentence_lengths = [len(tokens) for tokens in train[\"After_lemmatization\"]]\n",
        "TRAINING_VOCAB = sorted([(set(all_training_words))])\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Build testing vocabulary and get maximum testing sentence length and total number of words in testing data\n",
        "\n",
        "all_test_words = ''.join([word for tokens in test[\"After_lemmatization\"] for word in tokens])\n",
        "test_sentence_lengths = [len(tokens) for tokens in test[\"After_lemmatization\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN_gA-R74uLi",
        "outputId": "04231402-9e71-4bf7-bf61-cfdb6b187d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19745 words total, with a vocabulary size of 41\n",
            "Max sentence length is 117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)"
      ],
      "metadata": {
        "id": "NtLeMH6m42CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" -O \"/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwySWScVXtnZ",
        "outputId": "01da29f6-7f16-417f-f928-efacff492703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-11 02:46:48--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.4.110\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.4.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec_path = '/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=100000)"
      ],
      "metadata": {
        "id": "Zf10kf9j4zhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer.fit_on_texts(train[\"After_lemmatization\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(train[\"After_lemmatization\"].tolist())\n",
        "testing_sequences = tokenizer.texts_to_sequences(test[\"After_lemmatization\"].tolist())\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvMLsV8C45WL",
        "outputId": "a2d7ff62-6b78-4db6-eb69-604bbeb75c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9669 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = np.zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tvector = embedding.get(word)\n",
        "\t\tif vector is not None:\n",
        "\t\t\tweight_matrix[i] = vector\n",
        "\treturn weight_matrix"
      ],
      "metadata": {
        "id": "PJcVyljNa5nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 300"
      ],
      "metadata": {
        "id": "-wnNq8O0_dua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "train_cnn_data = pad_sequences(training_sequences, \n",
        "                               maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_cnn_data = pad_sequences(testing_sequences , \n",
        "                               maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "1JBmwHp751mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
        " EMBEDDING_DIM))"
      ],
      "metadata": {
        "id": "p62zhhbj52pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec word Embedding\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKrysajh54iY",
        "outputId": "4a89352a-6150-496c-a025-d57a07a22044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9670, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "ytrain = one_hot_encoder.fit_transform(train[\"Overall Sentiment\"].values.reshape(-1, 1))\n",
        "cnn_xtrain = train_cnn_data\n",
        "\n",
        "ytest = one_hot_encoder.transform(test[\"Overall Sentiment\"].values.reshape(-1, 1))\n",
        "cnn_xtest = test_cnn_data"
      ],
      "metadata": {
        "id": "8Jgo9QXnfOQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "def create_model(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "  embedding_layer = Embedding(num_words,\n",
        "                              embedding_dim,\n",
        "                              weights=[embeddings],\n",
        "                              input_length=max_sequence_length,\n",
        "                              trainable=False)\n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(Conv1D(filters=30, kernel_size=5, padding = \"valid\", activation='relu'))\n",
        "  model.add(Conv1D(10, kernel_size = 3, padding='valid', activation='relu', strides=1))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "metadata": {
        "id": "mWxTie8ebzkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(train_embedding_weights, \n",
        "                MAX_SEQUENCE_LENGTH, \n",
        "                len(train_word_index)+1, \n",
        "                EMBEDDING_DIM, \n",
        "                3)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc7vQnDAduIh",
        "outputId": "4b382110-6789-43ac-eba2-61373e39903e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 40, 300)           2901000   \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 36, 30)            45030     \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 34, 10)            910       \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 340)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               43648     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,999,039\n",
            "Trainable params: 98,039\n",
            "Non-trainable params: 2,901,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lACnhGyomGDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4e1e47-5626-49d3-c6c3-52a6c6e697db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0964 - accuracy: 0.4488\n",
            "Epoch 1: loss improved from inf to 1.09640, saving model to ./models/cnn-01-1.096-1.095.hdf5\n",
            "21/21 [==============================] - 2s 30ms/step - loss: 1.0964 - accuracy: 0.4488 - val_loss: 1.0948 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "16/21 [=====================>........] - ETA: 0s - loss: 1.0925 - accuracy: 0.4559\n",
            "Epoch 2: loss improved from 1.09640 to 1.09194, saving model to ./models/cnn-02-1.092-1.091.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0919 - accuracy: 0.4592 - val_loss: 1.0915 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "16/21 [=====================>........] - ETA: 0s - loss: 1.0887 - accuracy: 0.4556\n",
            "Epoch 3: loss improved from 1.09194 to 1.08793, saving model to ./models/cnn-03-1.088-1.089.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0879 - accuracy: 0.4592 - val_loss: 1.0886 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "17/21 [=======================>......] - ETA: 0s - loss: 1.0847 - accuracy: 0.4588\n",
            "Epoch 4: loss improved from 1.08793 to 1.08437, saving model to ./models/cnn-04-1.084-1.086.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0844 - accuracy: 0.4592 - val_loss: 1.0861 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "14/21 [===================>..........] - ETA: 0s - loss: 1.0807 - accuracy: 0.4664\n",
            "Epoch 5: loss improved from 1.08437 to 1.08137, saving model to ./models/cnn-05-1.081-1.084.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0814 - accuracy: 0.4592 - val_loss: 1.0838 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "17/21 [=======================>......] - ETA: 0s - loss: 1.0791 - accuracy: 0.4574\n",
            "Epoch 6: loss improved from 1.08137 to 1.07862, saving model to ./models/cnn-06-1.079-1.082.hdf5\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 1.0786 - accuracy: 0.4592 - val_loss: 1.0820 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "16/21 [=====================>........] - ETA: 0s - loss: 1.0759 - accuracy: 0.4613\n",
            "Epoch 7: loss improved from 1.07862 to 1.07629, saving model to ./models/cnn-07-1.076-1.080.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0763 - accuracy: 0.4592 - val_loss: 1.0802 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "16/21 [=====================>........] - ETA: 0s - loss: 1.0747 - accuracy: 0.4569\n",
            "Epoch 8: loss improved from 1.07629 to 1.07408, saving model to ./models/cnn-08-1.074-1.079.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0741 - accuracy: 0.4592 - val_loss: 1.0789 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "16/21 [=====================>........] - ETA: 0s - loss: 1.0717 - accuracy: 0.4622\n",
            "Epoch 9: loss improved from 1.07408 to 1.07220, saving model to ./models/cnn-09-1.072-1.078.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0722 - accuracy: 0.4592 - val_loss: 1.0776 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0708 - accuracy: 0.4583\n",
            "Epoch 10: loss improved from 1.07220 to 1.07051, saving model to ./models/cnn-10-1.071-1.077.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0705 - accuracy: 0.4592 - val_loss: 1.0767 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0690 - accuracy: 0.4592\n",
            "Epoch 11: loss improved from 1.07051 to 1.06902, saving model to ./models/cnn-11-1.069-1.076.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0690 - accuracy: 0.4592 - val_loss: 1.0758 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0720 - accuracy: 0.4453\n",
            "Epoch 12: loss improved from 1.06902 to 1.06782, saving model to ./models/cnn-12-1.068-1.075.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0678 - accuracy: 0.4592 - val_loss: 1.0751 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "16/21 [=====================>........] - ETA: 0s - loss: 1.0682 - accuracy: 0.4559\n",
            "Epoch 13: loss improved from 1.06782 to 1.06680, saving model to ./models/cnn-13-1.067-1.075.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0668 - accuracy: 0.4592 - val_loss: 1.0745 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0668 - accuracy: 0.4573\n",
            "Epoch 14: loss improved from 1.06680 to 1.06592, saving model to ./models/cnn-14-1.066-1.074.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0659 - accuracy: 0.4592 - val_loss: 1.0741 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "19/21 [==========================>...] - ETA: 0s - loss: 1.0649 - accuracy: 0.4600\n",
            "Epoch 15: loss improved from 1.06592 to 1.06519, saving model to ./models/cnn-15-1.065-1.074.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0652 - accuracy: 0.4592 - val_loss: 1.0736 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "14/21 [===================>..........] - ETA: 0s - loss: 1.0599 - accuracy: 0.4707\n",
            "Epoch 16: loss improved from 1.06519 to 1.06449, saving model to ./models/cnn-16-1.064-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0645 - accuracy: 0.4592 - val_loss: 1.0734 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0640 - accuracy: 0.4597\n",
            "Epoch 17: loss improved from 1.06449 to 1.06391, saving model to ./models/cnn-17-1.064-1.073.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0639 - accuracy: 0.4592 - val_loss: 1.0732 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0626 - accuracy: 0.4613\n",
            "Epoch 18: loss improved from 1.06391 to 1.06344, saving model to ./models/cnn-18-1.063-1.073.hdf5\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0634 - accuracy: 0.4592 - val_loss: 1.0731 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0595 - accuracy: 0.4683\n",
            "Epoch 19: loss improved from 1.06344 to 1.06306, saving model to ./models/cnn-19-1.063-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0631 - accuracy: 0.4592 - val_loss: 1.0730 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0627 - accuracy: 0.4592\n",
            "Epoch 20: loss improved from 1.06306 to 1.06272, saving model to ./models/cnn-20-1.063-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0627 - accuracy: 0.4592 - val_loss: 1.0730 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0598 - accuracy: 0.4657\n",
            "Epoch 21: loss improved from 1.06272 to 1.06246, saving model to ./models/cnn-21-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0625 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0667 - accuracy: 0.4493\n",
            "Epoch 22: loss improved from 1.06246 to 1.06226, saving model to ./models/cnn-22-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0623 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 5.0000e-04\n",
            "Epoch 23/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0626 - accuracy: 0.4583\n",
            "Epoch 23: loss improved from 1.06226 to 1.06218, saving model to ./models/cnn-23-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 1.0622 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 5.0000e-04\n",
            "Epoch 24/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0615 - accuracy: 0.4583\n",
            "Epoch 24: loss improved from 1.06218 to 1.06209, saving model to ./models/cnn-24-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0621 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 2.5000e-04\n",
            "Epoch 25/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0634 - accuracy: 0.4563\n",
            "Epoch 25: loss improved from 1.06209 to 1.06204, saving model to ./models/cnn-25-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0620 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 2.5000e-04\n",
            "Epoch 26/50\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.0624 - accuracy: 0.4585\n",
            "Epoch 26: loss improved from 1.06204 to 1.06201, saving model to ./models/cnn-26-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 1.0620 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.2500e-04\n",
            "Epoch 27/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0620 - accuracy: 0.4592\n",
            "Epoch 27: loss improved from 1.06201 to 1.06199, saving model to ./models/cnn-27-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0620 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.2500e-04\n",
            "Epoch 28/50\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.0622 - accuracy: 0.4588\n",
            "Epoch 28: loss improved from 1.06199 to 1.06198, saving model to ./models/cnn-28-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0620 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 6.2500e-05\n",
            "Epoch 29/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0609 - accuracy: 0.4600\n",
            "Epoch 29: loss improved from 1.06198 to 1.06196, saving model to ./models/cnn-29-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0620 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 6.2500e-05\n",
            "Epoch 30/50\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.0618 - accuracy: 0.4595\n",
            "Epoch 30: loss improved from 1.06196 to 1.06196, saving model to ./models/cnn-30-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0620 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 3.1250e-05\n",
            "Epoch 31/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0627 - accuracy: 0.4587\n",
            "Epoch 31: loss improved from 1.06196 to 1.06195, saving model to ./models/cnn-31-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0620 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 3.1250e-05\n",
            "Epoch 32/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0601 - accuracy: 0.4623\n",
            "Epoch 32: loss improved from 1.06195 to 1.06195, saving model to ./models/cnn-32-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0620 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.5625e-05\n",
            "Epoch 33/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 33: loss improved from 1.06195 to 1.06195, saving model to ./models/cnn-33-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.5625e-05\n",
            "Epoch 34/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 34: loss improved from 1.06195 to 1.06195, saving model to ./models/cnn-34-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 7.8125e-06\n",
            "Epoch 35/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0640 - accuracy: 0.4570\n",
            "Epoch 35: loss improved from 1.06195 to 1.06195, saving model to ./models/cnn-35-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 7.8125e-06\n",
            "Epoch 36/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0658 - accuracy: 0.4513\n",
            "Epoch 36: loss improved from 1.06195 to 1.06195, saving model to ./models/cnn-36-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 3.9063e-06\n",
            "Epoch 37/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 37: loss improved from 1.06195 to 1.06195, saving model to ./models/cnn-37-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 3.9063e-06\n",
            "Epoch 38/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 38: loss improved from 1.06195 to 1.06194, saving model to ./models/cnn-38-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.9531e-06\n",
            "Epoch 39/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 39: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-39-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.9531e-06\n",
            "Epoch 40/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0622 - accuracy: 0.4590\n",
            "Epoch 40: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-40-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 41/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 41: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-41-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 42/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0603 - accuracy: 0.4620\n",
            "Epoch 42: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-42-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0629 - accuracy: 0.4577\n",
            "Epoch 43: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-43-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.0612 - accuracy: 0.4605\n",
            "Epoch 44: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-44-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 45: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-45-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 15ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0634 - accuracy: 0.4547\n",
            "Epoch 46: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-46-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 47/50\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.0617 - accuracy: 0.4597\n",
            "Epoch 47: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-47-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 48/50\n",
            "15/21 [====================>.........] - ETA: 0s - loss: 1.0637 - accuracy: 0.4557\n",
            "Epoch 48: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-48-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 49/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 49: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-49-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n",
            "Epoch 50/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.4592\n",
            "Epoch 50: loss improved from 1.06194 to 1.06194, saving model to ./models/cnn-50-1.062-1.073.hdf5\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.0619 - accuracy: 0.4592 - val_loss: 1.0729 - val_accuracy: 0.4340 - lr: 1.0000e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f35905facd0>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "filepath = \"./models/cnn-{epoch:02d}-{loss:0.3f}-{val_loss:0.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "model.fit(cnn_xtrain, ytrain, batch_size=200, epochs=50, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(cnn_xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd7c91c-dcb1-48e1-ca9f-cec85f8b96d1",
        "id": "y9D9CXRmlLQ_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 50.503016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "B2MInuJYntX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')\n",
        "bow_train = bow_vectorizer.fit_transform(train[\"After_lemmatization\"])\n",
        "bow_test = bow_vectorizer.transform(test[\"After_lemmatization\"])"
      ],
      "metadata": {
        "id": "ZANqYNyyoU3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_softmax(model, data, bow):\n",
        "  dense_bow = bow.todense()\n",
        "  output = model.predict(data)\n",
        "  result = np.concatenate([dense_bow, output], axis=1)\n",
        "  return result"
      ],
      "metadata": {
        "id": "rSqwEd-gojAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_xtrain = concat_softmax(model, cnn_xtrain, bow_train)\n",
        "svm_xtest = concat_softmax(model, cnn_xtest, bow_test)"
      ],
      "metadata": {
        "id": "pPsJ90UXpr86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc = svm.SVC(kernel='rbf', gamma=0.5, C=0.1, probability=True).fit(svm_xtrain, train[\"Overall Sentiment\"].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9VYSOHisYO5",
        "outputId": "337b545e-b32f-4fc3-f057-1df38bc4fbdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = svc.predict(svm_xtest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSB3qbZ5uGrv",
        "outputId": "ba385ee9-a08a-4994-b152-77d9a6b324c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(test[\"Overall Sentiment\"], pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0fkLBosvG-T",
        "outputId": "021b1076-5ba2-4d5d-a435-84fa8a7e28c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5935613682092555"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_softmax_svm(cnn_model, cnn_data, svm_model, svm_data):\n",
        "  svm_output = svm_model.predict_proba(svm_data)\n",
        "  cnn_output = cnn_model.predict(cnn_data)\n",
        "  result = np.concatenate([cnn_output, svm_output], axis=1)\n",
        "  return result"
      ],
      "metadata": {
        "id": "CNpT4y01vfC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_svm_xtrain = concat_softmax_svm(model, cnn_xtrain, svc, svm_xtrain)\n",
        "final_svm_xtest = concat_softmax_svm(model, cnn_xtest, svc, svm_xtest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82STzNkGwF2q",
        "outputId": "8c72e673-cceb-4042-bc1b-493af927ed1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_svm_xtest.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFzTB39zyANL",
        "outputId": "215a1c30-7601-482c-bdcb-abe095898b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(497, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_svc = svm.SVC(kernel='rbf', gamma=0.5, C=0.1, probability=True).fit(final_svm_xtrain, train[\"Overall Sentiment\"].values)"
      ],
      "metadata": {
        "id": "AnhWf25gwmkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_pred = final_svc.predict(final_svm_xtest)\n",
        "accuracy_score(test[\"Overall Sentiment\"], final_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpQK5qBYxWzi",
        "outputId": "76d34c7c-069e-4c0f-cfd9-2958dd1e7104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6639839034205232"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(test[\"Overall Sentiment\"], final_pred, average=\"macro\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV1IeK8lyrt1",
        "outputId": "a393bb6b-ecda-4d96-907a-785c67642004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.634015063670394"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G2pyTR_3zDmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}